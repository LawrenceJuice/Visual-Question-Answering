This project involves developing a Visual Question Answering (VQA) model utilizing a pre-trained Swin Transformer for image feature extraction and a pre-trained BERT model for text encoding. The integrated visual and textual features are then fed into a classifier to predict the correct answers.

The pre-trained Swin Transformer is sourced from the torchvision library, while the pre-trained BERT model is from the transformers library.

Additionally, code from randaugment.py, originally from the BLIP project, was used to enhance the model's performance.

This repository was developed as the final project for the "深層学習/DL基礎 2024" (Deep Learning Basics 2024) course at the University of Tokyo.
